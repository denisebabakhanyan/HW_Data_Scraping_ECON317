{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW3_Problem2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc4Y_P253dnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install scrapy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIeduahqC67V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time \n",
        "import requests\n",
        "from scrapy.http import TextResponse "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw5ChfOgDFhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "url = \"https://staff.am/en/jobs\"\n",
        "base_url = \"https://staff.am\"\n",
        "page = requests.get(url)\n",
        "response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6TeGKyzRpKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining functions for companies, jobs, deadlines, locations, and individual pages\n",
        "# in order to scrape the first page of the website (used both css and xpath selectors).\n",
        "\n",
        "def get_company(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  company = response.css(\"p[class = 'job_list_company_title']::text\").extract()\n",
        "  return company \n",
        "  \n",
        "def get_job(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  job = response.css(\"p[class = 'font_bold']::text\").extract()\n",
        "  return job \n",
        "\n",
        "def get_deadline(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  deadline = response.xpath(\"//div[@class='job-inner job-list-deadline']/p[1]/text()\").extract()\n",
        "  deadline = [i.replace(\"\\n\",\"\") for i in deadline]\n",
        "  deadline_updated = [i for i in deadline if len(i)>0]\n",
        "  return deadline_updated\n",
        "\n",
        "def get_location(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  location = response.css(\"div[class = 'job-inner job-list-deadline'] > p[class = 'job_location']::text\").extract()\n",
        "  location = [i.replace(\"\\n\",\"\") for i in location]\n",
        "  location_updated = [i for i in location if len(i)>0]\n",
        "  return location_updated\n",
        "\n",
        "def get_individual_page(url):\n",
        "  page = requests.get(url)\n",
        "  response = TextResponse(url=page.url,body=page.text,encoding=\"utf-8\")\n",
        "  individual_page = [base_url + i for i in response.css(\"div[class = 'jobListButtonsBlock job_list_button_block'] > div[class = 'job-inner-right text-right load-more-container pull-right'] > a[class = 'load-more btn width100 job_load_more radius_changes']::attr(href)\").extract()]\n",
        "  return individual_page\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9FJ1KWIBY6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Writing a code all_pages in order to scrape all of the pages.\n",
        "# The amount of pages is equal to 12.\n",
        "all_pages = [\"https://staff.am/en/jobs?page={}&per-page=50\".format(i) for i in range(1,13)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb3b4601BaCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating empty lists and extending them with first scraped page in order \n",
        "# to receive all the necessary data for all pages. \n",
        "all_companies = []\n",
        "all_jobs = []\n",
        "all_deadlines = []\n",
        "all_locations = []\n",
        "all_individual_pages = []\n",
        "for i in all_pages:\n",
        "    all_companies.extend(get_company(i))\n",
        "    all_jobs.extend(get_job(i))\n",
        "    all_deadlines.extend(get_deadline(i))\n",
        "    all_locations.extend(get_location(i))\n",
        "    all_individual_pages.extend(get_individual_page(i))\n",
        "    time.sleep(1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkVJh9eXQbLO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a dataframe and turning it into csv file.\n",
        "df = pd.DataFrame(np.column_stack([all_companies, all_jobs, all_deadlines, all_locations, all_individual_pages]), columns = ['Companies', 'Jobs', 'Deadline', 'Location', 'Individual_Page'])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggyPbSbNRdRa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " df.to_csv('all_jobs.csv', index=False) "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjK5mssMRh7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv(\"all_jobs.csv\")"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWwh8ceaWpLs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "f6c63978-70b1-4714-e001-20f963b6eba7"
      },
      "source": [
        "# Question 1. Using value.counts() in order to see the most \n",
        "# popular company in terms of job posting count. \n",
        "data.Companies.value_counts().head(5)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Digitain              32\n",
              "SoftConstruct         29\n",
              "PicsArt               23\n",
              "ServiceTitan          16\n",
              "TeamViewer Armenia    12\n",
              "Name: Companies, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5l71JGPpcvsu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5b156ff4-3f5f-42a7-eb72-274d8ab9504e"
      },
      "source": [
        "# Question 2. Using str.lower() in order to lowercase the word \"data\"\n",
        "# on the website, and value_counts() and sum() in order to see how many jobs \n",
        "# have this word in it. \n",
        "data[data.Jobs.str.lower().str.contains(\"data\")][\"Jobs\"].value_counts().sum()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}